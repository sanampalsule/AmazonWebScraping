{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuc1eWJB2Om8"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "import smtplib\n",
        "import csv\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Website and pull in data\n",
        "\n",
        "URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data%2Banalyst%2Btshirt&qid=1626655184&sr=8-3&customId=B0752XJYNL&th=1'\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "\n",
        "title = soup2.find(id='productTitle').get_text()\n",
        "\n",
        "price = soup2.find(id='priceblock_ourprice').get_text()\n",
        "\n",
        "\n",
        "print(title)\n",
        "print(price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "mmwDcK5b2oAm",
        "outputId": "8bc3cd88-f32c-4e30-deda-da981fea9f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cdbc57a5f18c>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'productTitle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'priceblock_ourprice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price = price.strip()[1:]  # Remove leading/trailing white spaces and the dollar sign\n",
        "title = title.strip()  # Remove leading/trailing white spaces\n",
        "\n",
        "# Print the cleaned title and price\n",
        "print(title)\n",
        "print(price)\n",
        "\n",
        "# Get the current date\n",
        "today = datetime.date.today()\n",
        "\n",
        "# Print the current date\n",
        "print(today)\n"
      ],
      "metadata": {
        "id": "JjezjhI55vft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CSV file and write the headers and data\n",
        "header = ['Title', 'Price', 'Date']  # Define the headers for the CSV file\n",
        "data = [title, price, today]  # Create a list of data to be written\n",
        "\n",
        "# Open the CSV file in write mode and write the headers and data\n",
        "with open('AmazonWebScraperDataset.csv', 'w', newline='', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerow(data)\n",
        "\n",
        "# Read the CSV file using pandas and print its contents\n",
        "df = pd.read_csv(r'C:\\Users\\alexf\\AmazonWebScraperDataset.csv')\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "t-7y5Kkd501Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(data)"
      ],
      "metadata": {
        "id": "zDomja-S54fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all of the above code into one function\n",
        "\n",
        "\n",
        "def check_price():\n",
        "    URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data%2Banalyst%2Btshirt&qid=1626655184&sr=8-3&customId=B0752XJYNL&th=1'\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    page = requests.get(URL, headers=headers)\n",
        "\n",
        "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "\n",
        "    title = soup2.find(id='productTitle').get_text()\n",
        "\n",
        "    price = soup2.find(id='priceblock_ourprice').get_text()\n",
        "\n",
        "    price = price.strip()[1:]\n",
        "    title = title.strip()\n",
        "\n",
        "    import datetime\n",
        "\n",
        "    today = datetime.date.today()\n",
        "\n",
        "    import csv\n",
        "\n",
        "    header = ['Title', 'Price', 'Date']\n",
        "    data = [title, price, today]\n",
        "\n",
        "    with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(data)"
      ],
      "metadata": {
        "id": "z-ySy0KR6Dy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    check_price()\n",
        "    time.sleep(86400)  # Wait for 24 hours (86400 seconds) before the next execution\n",
        "\n",
        "# Read the updated CSV file using pandas and print its contents\n",
        "df = pd.read_csv(r'C:\\Users\\alexf\\AmazonWebScraperDataset.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "lUH_PrMB6EEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering and model training for time-series forecasting\n",
        "# Assuming the scraped data is stored in 'df' DataFrame\n",
        "X = df[['Price']]  # Use price as the feature\n",
        "y = df['Price'].shift(-7)  # Target variable is the price 7 days ahead\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Create and train the XGBoost model\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make recursive predictions for the next 7 days\n",
        "predictions = []\n",
        "last_price = X_test.iloc[-1]  # Start with the last known price\n",
        "for _ in range(7):\n",
        "    prediction = model.predict(last_price.values.reshape(1, -1))\n",
        "    predictions.append(prediction[0])\n",
        "    last_price = prediction  # Update the last price with the predicted value\n",
        "\n",
        "# Print the predicted prices for the next 7 days\n",
        "print(\"Predicted prices for the next 7 days:\")\n",
        "for i, price in enumerate(predictions):\n",
        "    print(f\"Day {i+1}: {price}\")"
      ],
      "metadata": {
        "id": "L_uV_rhD6NL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}